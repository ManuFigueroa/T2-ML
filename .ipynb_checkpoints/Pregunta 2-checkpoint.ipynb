{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 2. Análisis de opiniones sobre películas.\n",
    "a) En primer lugar hay que descargar los datos de la URL asignada y luego se guardan en los archivos train_data2.csv y test_data2.csv. \n",
    "\n",
    "Luego se abren los archivos y se crea un dataframe para los conjuntos de datos de entrenamiento (train) y prueba (test).\n",
    "\n",
    "La primera columna de los sets de datos corresponde al sentimiento (Sentiment). Si es \"-1\", entonces el comentario es negativo. Si es \"+1\" entonces el comentario será positivo. Esto se pudo comprobar leyendo un extracto del dataframe. \n",
    "\n",
    "La segunda columna será el review de la persona con respecto a una película."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3554, 2)\n",
      "(3554, 2)\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data2.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data2.csv\")\n",
    "'''\n",
    "\n",
    "\n",
    "ftr = open(\"train_data2.csv\", \"r\")\n",
    "fts = open(\"test_data2.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "print train_df.shape\n",
    "print test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se tiene que tanto el conjunto de entrenamiento como el de pruebas tienen igual cantidad de registros (3554 filas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Ahora hay que crear una función que entregue una lista de palabras a partir de un determinado texto. A continuación se muestra el código del enunciado, en el cual realiza lematización. Notar que la función ya incluye su debida transformación a lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "def word_extractor_lemmatizer(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print word_extractor_lemmatizer(\"I love to eat cake\")\n",
    "print word_extractor_lemmatizer(\"I love eating cake\")\n",
    "print word_extractor_lemmatizer(\"I loved eating the cake\")\n",
    "print word_extractor_lemmatizer(\"I do not love eating cake\")\n",
    "print word_extractor_lemmatizer(\"I don't love eating cake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código es el modificado que incluye el proceso de stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " n't love eat cake\n"
     ]
    }
   ],
   "source": [
    "def word_extractor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ stemmer.stem(word.lower()) for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I don't love eating cake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una gran diferencia ya que, para la función que incluye lematización, se puede ver a simple vista que solo se eliminaron las stop-words. En cambio, para la función con stemming, las primeras cuatro frases quedan como \"love eat cake\", ya que las palabras \"loved\" y \"eating\" se redujeron a su raiz \"love\" y \"eat\" respectivamente.\n",
    "\n",
    "Probando con otras frases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'm play playstat 4 everi week\n",
      " play playstat 4 everi week\n",
      " learn lot machin learn class\n",
      " learn lot machin learn class\n"
     ]
    }
   ],
   "source": [
    "print word_extractor(\"I'm playing PlayStation 4 every week\")\n",
    "print word_extractor(\"I play PlayStation 4 every week\")\n",
    "\n",
    "print word_extractor(\"I learned a lot in the Machine Learning class\")\n",
    "print word_extractor(\"I learn a lot from the Machine Learning classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los ejemplos anteriores, se ve que para las dos primeras frases la función trata al nombre propio \"PlayStation\" como una palabra más, y le borra el sufijo \"ion\", quedando \"playstat\". \n",
    "\n",
    "Para las otras dos frases, cada una entrega un mensaje distinto. La primera comunica que el sujeto aprendió harto de la clase de Machine Learning. En la segunda frase, el sujeto informa que aprende harto de las clases de Machine Learning. Aún así, luego del proceso de stemming ambas frases quedan iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) La función word_extractor_lemmatizer() trata las palabras por lematización y fue realizada anteriormente. A continuación se muestran los últimos ejemplos anteriores utilizando lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'm playing playstation 4 every week\n",
      " play playstation 4 every week\n",
      " learned lot machine learning class\n",
      " learn lot machine learning class\n",
      " better\n"
     ]
    }
   ],
   "source": [
    "print word_extractor_lemmatizer(\"I'm playing PlayStation 4 every week\")\n",
    "print word_extractor_lemmatizer(\"I play PlayStation 4 every week\")\n",
    "\n",
    "print word_extractor_lemmatizer(\"I learned a lot in the Machine Learning class\")\n",
    "print word_extractor_lemmatizer(\"I learn a lot from the Machine Learning classes\")\n",
    "\n",
    "print word_extractor_lemmatizer(\"better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Técnicamente, el proceso de stemming lo que hace es cortar el final de las palabras sin importar el significado de la palabra según el contexto. Lemmatization (o lematización) es un proceso mas complejo, ya que utiliza una búsqueda en un vocabulario y realiza un análisis morfológico de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-40-e297d9877bb1>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-40-e297d9877bb1>\"\u001b[1;36m, line \u001b[1;32m24\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "'''def vector(tipo='lemmatizer'):\n",
    "  if (tipo == 'lemmatizer'):'''\n",
    "texts_train = [word_extractor_lemmatizer(text) for text in train_df.Text]\n",
    "texts_test = [word_extractor_lemmatizer(text) for text in test_df.Text]\n",
    "'''elif tipo == 'stemming':\n",
    "    texts_train = [word_extractor(text) for text in train_df.Text]\n",
    "    texts_test = [word_extractor(text) for text in test_df.Text]'''\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0)\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
