{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 2. Análisis de opiniones sobre películas.\n",
    "a) En primer lugar hay que descargar los datos de la URL asignada y luego se guardan en los archivos train_data2.csv y test_data2.csv. \n",
    "\n",
    "Luego se abren los archivos y se crea un dataframe para los conjuntos de datos de entrenamiento (train) y prueba (test).\n",
    "\n",
    "La primera columna de los sets de datos corresponde al sentimiento (Sentiment). Si es \"-1\", entonces el comentario es negativo. Si es \"+1\" entonces el comentario será positivo. Esto se pudo comprobar leyendo un extracto del dataframe. \n",
    "\n",
    "La segunda columna será el review de la persona con respecto a una película."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3554, 2)\n",
      "(3554, 2)\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data2.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data2.csv\")\n",
    "'''\n",
    "\n",
    "\n",
    "ftr = open(\"train_data2.csv\", \"r\")\n",
    "fts = open(\"test_data2.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "print train_df.shape\n",
    "print test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se tiene que tanto el conjunto de entrenamiento como el de pruebas tienen igual cantidad de registros (3554 filas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Ahora hay que crear una función que entregue una lista de palabras a partir de un determinado texto. A continuación se muestra el código del enunciado, en el cual realiza lematización. Notar que la función ya incluye su debida transformación a lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def word_extractor_lemmatizer(text, del_stopwords=True):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    if del_stopwords == True:\n",
    "        commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if del_stopwords==True:\n",
    "            if word not in commonwords:\n",
    "                words+=\" \"+word\n",
    "        else:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print word_extractor_lemmatizer(\"I love to eat cake\")\n",
    "print word_extractor_lemmatizer(\"I love eating cake\")\n",
    "print word_extractor_lemmatizer(\"I loved eating the cake\")\n",
    "print word_extractor_lemmatizer(\"I do not love eating cake\")\n",
    "print word_extractor_lemmatizer(\"I don't love eating cake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código es el modificado que incluye el proceso de stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " n't love eat cake\n"
     ]
    }
   ],
   "source": [
    "def word_extractor(text, del_stopwords=True):\n",
    "    stemmer = PorterStemmer()\n",
    "    if del_stopwords==True:\n",
    "        commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ stemmer.stem(word.lower()) for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if del_stopwords==True:\n",
    "            if word not in commonwords:\n",
    "                words+=\" \"+word\n",
    "        else:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I don't love eating cake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una gran diferencia ya que, para la función que incluye lematización, se puede ver a simple vista que solo se eliminaron las stop-words. En cambio, para la función con stemming, las primeras cuatro frases quedan como \"love eat cake\", ya que las palabras \"loved\" y \"eating\" se redujeron a su raiz \"love\" y \"eat\" respectivamente.\n",
    "\n",
    "Probando con otras frases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'm play playstat 4 everi week\n",
      " play playstat 4 everi week\n",
      " learn lot machin learn class\n",
      " learn lot machin learn class\n"
     ]
    }
   ],
   "source": [
    "print word_extractor(\"I'm playing PlayStation 4 every week\")\n",
    "print word_extractor(\"I play PlayStation 4 every week\")\n",
    "\n",
    "print word_extractor(\"I learned a lot in the Machine Learning class\")\n",
    "print word_extractor(\"I learn a lot from the Machine Learning classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los ejemplos anteriores, se ve que para las dos primeras frases la función trata al nombre propio \"PlayStation\" como una palabra más, y le borra el sufijo \"ion\", quedando \"playstat\". \n",
    "\n",
    "Para las otras dos frases, cada una entrega un mensaje distinto. La primera comunica que el sujeto aprendió harto de la clase de Machine Learning. En la segunda frase, el sujeto informa que aprende harto de las clases de Machine Learning. Aún así, luego del proceso de stemming ambas frases quedan iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) La función word_extractor_lemmatizer() trata las palabras por lematización y fue realizada anteriormente. A continuación se muestran los últimos ejemplos anteriores utilizando lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'm playing playstation 4 every week\n",
      " play playstation 4 every week\n",
      " learned lot machine learning class\n",
      " learn lot machine learning class\n"
     ]
    }
   ],
   "source": [
    "print word_extractor_lemmatizer(\"I'm playing PlayStation 4 every week\")\n",
    "print word_extractor_lemmatizer(\"I play PlayStation 4 every week\")\n",
    "\n",
    "print word_extractor_lemmatizer(\"I learned a lot in the Machine Learning class\")\n",
    "print word_extractor_lemmatizer(\"I learn a lot from the Machine Learning classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Técnicamente, el proceso de stemming lo que hace es cortar el final de las palabras sin importar el significado de la palabra según el contexto. Lemmatization (o lematización) es un proceso mas complejo, ya que utiliza una búsqueda en un vocabulario y realiza un análisis morfológico de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Ahora hay que realizar una representación vectorial de los textos de entrenamiento y pruebas. En primer lugar, los textos se pasan por el proceso de lematización, eliminando también las *stop words*. Luego, se inicializa la función *CountVectorizer* con n=1, de tal forma que cada palabra sumará 1. Esto es útil cuando se suma el total de la presencia de una palabra. Estos vectores quedan guardados en *features_train* y *features_test* para los sets de entrenamiento y prueba respectivamente.\n",
    "\n",
    "Luego se guarda la etiqueta de cada review. Cuando el campo Sentiment es -1 y se suma con 1, queda en 0. Si el mismo campo es 1 y se suma con 1, luego se divide por 2 y queda 1. Entonces, las etiquetas quedarán guardadas como 0 cuando la review es negativa y 1 cuando la review es positiva. Estos datos quedan guardados en las variables *labels_train* y *labels_test*.\n",
    "\n",
    "Luego se guardan las palabras en el vector *vocab*, y su frecuencia de aparición en el vector *dist*. Esto es usado más abajo para determinar cuáles son las palabras mas frecuentes tanto en el set de entrenamiento como en el de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def vectorial(modo='lemmatization', del_stopwords=True, print_top10=0):\n",
    "    \n",
    "    if modo=='lemmatization':\n",
    "        if del_stopwords==True:\n",
    "            texts_train = [word_extractor_lemmatizer(text) for text in train_df.Text]\n",
    "            texts_test = [word_extractor_lemmatizer(text) for text in test_df.Text]\n",
    "        else:\n",
    "            texts_train = [word_extractor_lemmatizer(text, False) for text in train_df.Text]\n",
    "            texts_test = [word_extractor_lemmatizer(text, False) for text in test_df.Text]\n",
    "    if modo == 'stemming':\n",
    "        if del_stopwords==True:\n",
    "            texts_train = [word_extractor(text) for text in train_df.Text]\n",
    "            texts_test = [word_extractor(text) for text in test_df.Text]\n",
    "        else:\n",
    "            texts_train = [word_extractor(text, False) for text in train_df.Text]\n",
    "            texts_test = [word_extractor(text, False) for text in test_df.Text]\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "    vectorizer.fit(np.asarray(texts_train))\n",
    "    features_train = vectorizer.transform(texts_train)\n",
    "    features_test = vectorizer.transform(texts_test)\n",
    "    labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0)\n",
    "    labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0)\n",
    "\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    dist_train=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "    if print_top10 == 1:\n",
    "        rank = []\n",
    "        for tag, count in zip(vocab, dist_train):\n",
    "            rank.append((count, tag))\n",
    "\n",
    "        rank.sort()\n",
    "        rank[:] = rank[::-1]\n",
    "        \n",
    "        print \"MODO:\", modo, \"ELIMINA stopwords:\", del_stopwords, \"\\n\"\n",
    "        \n",
    "        print \"Palabras más frecuentes en el set de entrenamiento\"\n",
    "        for i in range(0,10):\n",
    "            print \"Frecuencia: %d, palabra: %s\"% (rank[i][0], rank[i][1])\n",
    "\n",
    "        vocab = vectorizer.get_feature_names()\n",
    "        dist_test=list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "        rank = []\n",
    "        for tag, count in zip(vocab, dist_test):\n",
    "            rank.append((count, tag))\n",
    "\n",
    "        rank.sort()\n",
    "        rank[:] = rank[::-1]\n",
    "        print \"\\nPalabras más frecuentes en el set de prueba\"\n",
    "        for i in range(0,10):\n",
    "            print \"Frecuencia: %d, palabra: %s\"% (rank[i][0], rank[i][1])\n",
    "    return features_train, labels_train, features_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODO: lematize ELIMINA stopwords: True \n",
      "\n",
      "Palabras más frecuentes en el set de entrenamiento\n",
      "Frecuencia: 566, palabra: film\n",
      "Frecuencia: 481, palabra: movie\n",
      "Frecuencia: 246, palabra: one\n",
      "Frecuencia: 245, palabra: like\n",
      "Frecuencia: 224, palabra: ha\n",
      "Frecuencia: 183, palabra: make\n",
      "Frecuencia: 176, palabra: story\n",
      "Frecuencia: 163, palabra: character\n",
      "Frecuencia: 145, palabra: comedy\n",
      "Frecuencia: 143, palabra: time\n",
      "\n",
      "Palabras más frecuentes en el set de prueba\n",
      "Frecuencia: 558, palabra: film\n",
      "Frecuencia: 540, palabra: movie\n",
      "Frecuencia: 250, palabra: one\n",
      "Frecuencia: 238, palabra: ha\n",
      "Frecuencia: 230, palabra: like\n",
      "Frecuencia: 197, palabra: story\n",
      "Frecuencia: 175, palabra: character\n",
      "Frecuencia: 165, palabra: time\n",
      "Frecuencia: 161, palabra: make\n",
      "Frecuencia: 134, palabra: comedy\n"
     ]
    }
   ],
   "source": [
    "vectorial(print_top10=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) La función *classification_report* de *sklearn* retorna precision, recall y F1 score. Estos indicadores se definen como siguen:\n",
    "* precision: Cantidad de verdaderos positivos dividido por la cantidad de resultados positivos obtenidos (falsos y verdaderos juntos).\n",
    "* recall: Cantidad de verdaderos positivos obtenidos dividido por la cantidad de positivos que se deberían haber obtenido.\n",
    "* F1 score: Corresponde a la media armónica entre precision y recall. Se calcula de la siguiente forma: $F1 Score = 2\\frac{precision * recall}{precison + recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy %s: %f\"%(text,acc_tr)\n",
    "    print \"Test Accuracy %s: %f\"%(text,acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) A continuación se muestra el código que realiza el modelo con el algoritmo **Bernoulli naive Bayes** (binario). Se ejecuta la función *vectorial(modo=..., del_stopwords=..., print_top10=...)* de forma que el retorno de la función cargue los datos de features y labels según el tipo de filtro de palabras que se quiera ocupar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization sin stop words\n",
      "Training Accuracy BernoulliNB: 0.958638\n",
      "Test Accuracy BernoulliNB: 0.738531\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Muestra aleatoria del conjunto de prueba (test), con sus respectivas probabilidades de naive Bayes\n",
      "[ 0.9847833  0.0152167] while not all that bad of a movie , it's nowhere near as good as the original .\n",
      "\n",
      "[ 0.08593455  0.91406545] this is an extraordinary film , not least because it is japanese and yet feels universal .\n",
      "\n",
      "[ 0.55574848  0.44425152] what doesn't this film have that an impressionable kid couldn't stand to hear ?\n",
      "\n",
      "[ 0.56785683  0.43214317] the actresses may have worked up a back story for the women they portray so convincingly , but viewers don't get enough of that background for the characters to be involving as individuals rather than types .\n",
      "\n",
      "[ 0.90083668  0.09916332] take away all the cliches and the carbon copy scenes from every drug movie we've seen and all you have left are john leguizamo's cool jackets .\n",
      "\n",
      "[ 0.17313177  0.82686823] not too fancy , not too filling , not too fluffy , but definitely tasty and sweet .\n",
      "\n",
      "[ 0.83827618  0.16172382] at the one-hour mark , herzog simply runs out of ideas and the pace turns positively leaden as the movie sputters to its inevitable tragic conclusion .\n",
      "\n",
      "[  8.09350690e-04   9.99190649e-01] a tender and touching drama , based on the true story of a troubled african-american's quest to come to terms with his origins , reveals the yearning we all have in our hearts for acceptance within the family circle .\n",
      "\n",
      "[ 0.03386733  0.96613267] 'compleja e intelectualmente retadora , el ladr�n de orqu�deas es uno de esos filmes que vale la pena ver precisamente por su originalidad . '\n",
      "\n",
      "[ 0.8361165  0.1638835] a technically well-made suspenser . . . but its abrupt drop in iq points as it races to the finish line proves simply too discouraging to let slide .\n",
      "\n",
      "\n",
      "Lemmatization con stop words\n",
      "Training Accuracy BernoulliNB: 0.955262\n",
      "Test Accuracy BernoulliNB: 0.748663\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.76      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "\n",
      "Stemming sin stop words\n",
      "Training Accuracy BernoulliNB: 0.942881\n",
      "Test Accuracy BernoulliNB: 0.747819\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "\n",
      "Stemming con stop words\n",
      "Training Accuracy BernoulliNB: 0.938098\n",
      "Test Accuracy BernoulliNB: 0.762173\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.77      0.76      0.77      1803\n",
      "          -       0.76      0.76      0.76      1751\n",
      "\n",
      "avg / total       0.76      0.76      0.76      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import random\n",
    "\n",
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "\n",
    "print \"Lemmatization sin stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='lemmatization')\n",
    "model = do_NAIVE_BAYES(features_train, labels_train, features_test, labels_test)\n",
    "print \"Muestra aleatoria del conjunto de prueba (test), con sus respectivas probabilidades de naive Bayes\\n\"\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 5)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text\n",
    "\n",
    "print \"Lemmatization con stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='lemmatization', del_stopwords=False)\n",
    "model = do_NAIVE_BAYES(features_train, labels_train, features_test, labels_test)\n",
    "    \n",
    "print \"Stemming sin stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='stemming', del_stopwords=True)\n",
    "model = do_NAIVE_BAYES(features_train, labels_train, features_test, labels_test)\n",
    "\n",
    "print \"Stemming con stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='stemming', del_stopwords=False)\n",
    "model = do_NAIVE_BAYES(features_train, labels_train, features_test, labels_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Como resumen de los resultados anteriores para Bernoulli naive Bayes :\n",
    "\n",
    "Lemmatization sin stop words:\n",
    "* Training accuracy: 0.959\n",
    "* Test accuracy: 0.739\n",
    "* Precision: 0.740\n",
    "* Recall: 0.740\n",
    "* F1-Score: 0.740\n",
    "\n",
    "Lemmatization con stop words:\n",
    "* Training accuracy: 0.955\n",
    "* Test accuracy: 0.748\n",
    "* Precision: 0.750\n",
    "* Recall: 0.750\n",
    "* F1-Score: 0.750\n",
    "\n",
    "Stemming sin stop words:\n",
    "* Training accuracy: 0.943\n",
    "* Test accuracy: 0.748\n",
    "* Precision: 0.750\n",
    "* Recall: 0.750\n",
    "* F1-Score: 0.750\n",
    "\n",
    "Stemming con stop words:\n",
    "* Training accuracy: 0.938\n",
    "* Test accuracy: 0.762\n",
    "* Precision: 0.760\n",
    "* Recall: 0.760\n",
    "* F1-Score: 0.760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVISAR POR FAVOR\n",
    "\n",
    "Como se puede ver en las métricas obtenidas (precision, recall y F1-score), se puede observar que todos los modelos generados, tanto con lemmatization como con stemming, no difieren mucho entre ellos. Sin embargo, si se observan los ajustes realizados sin stop words, el método de stemming es mejor que el de lemmatization, ya que presenta scores de precision, recall y F1-score mejores. Aún así, como los *scores* de cada modelo no difieren mucho, convendría usar la forma más rápida.\n",
    "\n",
    "Con respecto a los ejemplos aleatorios, se observa que las probabilidades están muy bien repartidas, ya que la mayoría tiene probabilidad > 0.8 para una clase, y probabilidad < 0.2 para la otra clase. Esto no implica que el modelo clasifique correctamente cada uno de los elementos de prueba. Por ejemplo, al momento de escribir este párrafo se encontraron las siguientes clasificaciones:\n",
    "\n",
    "    1. [ 0.55574848  0.44425152] what doesn't this film have that an impressionable kid couldn't stand to hear ?\n",
    "    2. [ 0.56785683  0.43214317] the actresses may have worked up a back story for the women they portray so convincingly , but viewers don't get enough of that background for the characters to be involving as individuals rather than types .\n",
    "\n",
    "Para ambas reviews se asignaron probabilidades de manera similar, por lo que es difícil decidir a qué clase corresponde cada una. Si se decide ir con la probabilidad mayor, entonces la review 1 quedaría mal clasificada. Esto puede deberse a que la review no es muy explicativa como opinión, sino que corresponde a una pregunta retórica. \n",
    "\n",
    "# REVISAR \n",
    "Además, notar que el training accuracy siempre es mejor que el test accuracy. Esto se debe a que en el set de prueba (test) existen palabras que no están presentes en el set de etrenamiento, y el modelo no se encuentra preparado para evaluar aquellas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Ahora hay que crear un modelo utilizando **Multinomial naive Bayes** (MNB). A continuación se realiza el mismo procedimiento que el anterior, pero utilizando este algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization sin stop words\n",
      "Training Accuracy MultinomialNB: 0.959482\n",
      "Test Accuracy MultinomialNB: 0.740782\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Muestra aleatoria del conjunto de prueba (test), con sus respectivas probabilidades de naive Bayes\n",
      "[ 0.97587476  0.02412524] the backyard battles you staged with your green plastic army men were more exciting and almost certainly made more sense .\n",
      "\n",
      "[  1.72450066e-04   9.99827550e-01] what makes salton sea surprisingly engrossing is that caruso takes an atypically hypnotic approach to a world that's often handled in fast-edit , hopped-up fashion .\n",
      "\n",
      "[ 0.3719863  0.6280137] . . . would be a total loss if not for two supporting performances taking place at the movie's edges .\n",
      "\n",
      "[ 0.92753966  0.07246034] a sharp and quick documentary that is funny and pithy , while illuminating an era of theatrical comedy that , while past , really isn't .\n",
      "\n",
      "[ 0.14197731  0.85802269] 'frailty \" starts out like a typical bible killer story , but it turns out to be significantly different ( and better ) than most films with this theme .\n",
      "\n",
      "\n",
      "Lemmatization con stop words\n",
      "Training Accuracy MultinomialNB: 0.955543\n",
      "Test Accuracy MultinomialNB: 0.747537\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.75      0.75      1803\n",
      "          -       0.74      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "\n",
      "Stemming sin stop words\n",
      "Training Accuracy MultinomialNB: 0.942319\n",
      "Test Accuracy MultinomialNB: 0.749789\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.75      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "\n",
      "Stemming con stop words\n",
      "Training Accuracy MultinomialNB: 0.940630\n",
      "Test Accuracy MultinomialNB: 0.759921\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.77      0.76      1803\n",
      "          -       0.76      0.75      0.76      1751\n",
      "\n",
      "avg / total       0.76      0.76      0.76      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def do_MULTINOMIAL(x,y,xt,yt):\n",
    "    model = MultinomialNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"MultinomialNB\")\n",
    "    return model\n",
    "\n",
    "print \"Lemmatization sin stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='lemmatization')\n",
    "model = do_MULTINOMIAL(features_train, labels_train, features_test, labels_test)\n",
    "print \"Muestra aleatoria del conjunto de prueba (test), con sus respectivas probabilidades de naive Bayes\\n\"\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 5)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text\n",
    "\n",
    "print \"Lemmatization con stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='lemmatization', del_stopwords=False)\n",
    "model = do_MULTINOMIAL(features_train, labels_train, features_test, labels_test)\n",
    "    \n",
    "print \"\\nStemming sin stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='stemming', del_stopwords=True)\n",
    "model = do_MULTINOMIAL(features_train, labels_train, features_test, labels_test)\n",
    "\n",
    "print \"\\nStemming con stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='stemming', del_stopwords=False)\n",
    "model = do_MULTINOMIAL(features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen de los resultados anteriores para Multinomial naive Bayes :\n",
    "\n",
    "Lemmatization sin stop words:\n",
    "* Training accuracy: 0.959\n",
    "* Test accuracy: 0.741\n",
    "* Precision: 0.740\n",
    "* Recall: 0.740\n",
    "* F1-Score: 0.740\n",
    "\n",
    "Lemmatization con stop words:\n",
    "* Training accuracy: 0.956\n",
    "* Test accuracy: 0.748\n",
    "* Precision: 0.750\n",
    "* Recall: 0.750\n",
    "* F1-Score: 0.750\n",
    "\n",
    "Stemming sin stop words:\n",
    "* Training accuracy: 0.942\n",
    "* Test accuracy: 0.749\n",
    "* Precision: 0.750\n",
    "* Recall: 0.750\n",
    "* F1-Score: 0.750\n",
    "\n",
    "Stemming con stop words:\n",
    "* Training accuracy: 0.941\n",
    "* Test accuracy: 0.759\n",
    "* Precision: 0.760\n",
    "* Recall: 0.760\n",
    "* F1-Score: 0.760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual como sucede con Bernoulli naive Bayes, los modelos ajustados con Multinomial naive Bayes son muy similares independiente de la forma en como se procese el texto. Aún así, el método de stemming sigue siendo mejor que lemmatization.\n",
    "\n",
    "Los ejemplos aleatorios obtenidos son los siguientes:\n",
    "\n",
    "    1. [ 0.97587476  0.02412524] the backyard battles you staged with your green plastic army men were more exciting and almost certainly made more sense .\n",
    "    2. [  1.72450066e-04   9.99827550e-01] what makes salton sea surprisingly engrossing is that caruso takes an atypically hypnotic approach to a world that's often handled in fast-edit , hopped-up fashion .\n",
    "    3. [ 0.3719863  0.6280137] . . . would be a total loss if not for two supporting performances taking place at the movie's edges .\n",
    "    4. [ 0.92753966  0.07246034] a sharp and quick documentary that is funny and pithy , while illuminating an era of theatrical comedy that , while past , really isn't .\n",
    "    5. [ 0.14197731  0.85802269] 'frailty \" starts out like a typical bible killer story , but it turns out to be significantly different ( and better ) than most films with this theme .\n",
    "\n",
    "# REVISAR\n",
    "Se puede observar claramente que las probabilidades de la opinión 4 están completamente erroneas, ya que la opinión es positiva. Las probabilidades de la opinión 3 se encuentran más cerca de 0.5 que de 1.0, lo que no es confiable ya que sería difícil decidir a cual clase pertenece. Además, esta opinión es negativa, y la probabilidad mayor corresponde a la clase positiva. El resto de las opiniones se encuentra bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Para esta parte hay que ajustar un modelo de regresión logística regularizado, utilizando la norma $l_2$. En el código del enunciado se realiza un modelo para un listado con distintos valores de regularización C. Luego de observar como cambia el modelo para cada uno de los valores de C, se pudo ver que para los valores más grandes, el modelo tiende a *overfitting*. En cambio, cuando C es más pequeño, el modelo no es tan fuerte. Es por esta razón que se decidió usar un parámetro de regularización C igual a 0.5. \n",
    "\n",
    "# NO ESTOY SEGURO DE ESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization sin stop words\n",
      "\n",
      "Training Accuracy LOGISTIC: 0.972988\n",
      "Test Accuracy LOGISTIC: 0.725584\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.73      1803\n",
      "          -       0.72      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "[ 0.72220716  0.27779284] crudup's screen presence is the one thing that holds interest in the midst of a mushy , existential exploration of why men leave their families .\n",
      "\n",
      "[ 0.02792055  0.97207945] the best didacticism is one carried by a strong sense of humanism , and bertrand tavernier's oft-brilliant safe conduct ( \" laissez-passer \" ) wears its heart on its sleeve .\n",
      "\n",
      "[ 0.32116462  0.67883538] it's not a classic spy-action or buddy movie , but it's entertaining enough and worth a look .\n",
      "\n",
      "[ 0.59770433  0.40229567] steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "[ 0.7361555  0.2638445] john carlen's script is full of unhappy , two-dimensional characters who are anything but compelling .\n",
      "\n",
      "Lemmatization con stop words\n",
      "\n",
      "Training Accuracy LOGISTIC: 0.968205\n",
      "Test Accuracy LOGISTIC: 0.732902\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.73      0.74      1803\n",
      "          -       0.73      0.74      0.73      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "\n",
      "Stemming sin stop words\n",
      "\n",
      "Training Accuracy LOGISTIC: 0.960045\n",
      "Test Accuracy LOGISTIC: 0.737124\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.74      0.74      1803\n",
      "          -       0.73      0.73      0.73      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "\n",
      "Stemming con stop words\n",
      "\n",
      "Training Accuracy LOGISTIC: 0.960045\n",
      "Test Accuracy LOGISTIC: 0.743034\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.74      0.75      1803\n",
      "          -       0.74      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def do_LOGIT(x,y,xt,yt):\n",
    "    start_t = time.time()\n",
    "    '''Cs = [0.01,0.1,10,100,1000]\n",
    "    for C in Cs:'''\n",
    "    '''print \"Usando C= %f\"%C'''\n",
    "    model = LogisticRegression(penalty='l2',C=0.5)\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "    return model\n",
    "        \n",
    "print \"Lemmatization sin stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='lemmatization')\n",
    "model = do_LOGIT(features_train, labels_train, features_test, labels_test)  \n",
    "test_pred = model.predict_proba(features_test)\n",
    "\n",
    "spl = random.sample(xrange(len(test_pred)), 5)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text\n",
    "\n",
    "print \"Lemmatization con stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='lemmatization', del_stopwords=False)\n",
    "model = do_LOGIT(features_train, labels_train, features_test, labels_test)\n",
    "    \n",
    "print \"\\nStemming sin stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='stemming', del_stopwords=True)\n",
    "model = do_LOGIT(features_train, labels_train, features_test, labels_test)\n",
    "\n",
    "print \"\\nStemming con stop words\\n\"\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = vectorial(modo='stemming', del_stopwords=False)\n",
    "model = do_LOGIT(features_train, labels_train, features_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa un comportamiento similar a los ajustes anteriores. Los *scores* indican que el método de stemming es mejor que lemmatization. El hecho de borrar las stop words puede significar un ahorro en tiempo, pero no necesariamente sirve para obtener mejores resultados. De hecho, aquellas palabras, al no expresar sentimientos, pueden causar que el modelo clasifique erroneamente.\n",
    "\n",
    "Los ejemplos aleatorios obtenidos fueron los siguientes:\n",
    "    \n",
    "    1. [ 0.72220716  0.27779284] crudup's screen presence is the one thing that holds interest in the midst of a mushy , existential exploration of why men leave their families .\n",
    "    2. [ 0.02792055  0.97207945] the best didacticism is one carried by a strong sense of humanism , and bertrand tavernier's oft-brilliant safe conduct ( \" laissez-passer \" ) wears its heart on its sleeve .\n",
    "    3. [ 0.32116462  0.67883538] it's not a classic spy-action or buddy movie , but it's entertaining enough and worth a look .\n",
    "    4. [ 0.59770433  0.40229567] steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
    "    5. [ 0.7361555  0.2638445] john carlen's script is full of unhappy , two-dimensional characters who are anything but compelling .\n",
    "\n",
    "# Revisar por favor\n",
    "Se puede observar que para la mayoría, las probabilidades de cada clase han disminuido en la decena, es decir si antes la probabilidad de una clase era > 0.8, ahora tiende a ser > 0.7. Además, si se comparan estos resultados con la clasificación real, se puede ver que la opinión 4 tiene una mayor probabilidad en la clase negativa, siendo que la opinión es positiva. Esto puede deberse a la presencia de la palabra *hate*, a pesar de que se utiliza para probar un punto positivo en la opinión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
